# -*- coding: utf-8 -*-
"""golbal_web_scrapper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jHFnI2_o-aSG8esv31GcFcIY9uSxxZt5

## global web scrapper
"""

# install imported Func
!pip install tldextract

## function to extract info
def domain_extract(url):
    ext = tldextract.extract(url)
    return ext.domain

def get_tag_text(url, page_source_text, tag_text):
    text_dict={}
    tag_text_list = page_source_text.find_all(tag_text)
    tag_texts = [div.text for div in tag_text_list]
    text_dict[domain_extract(url)] = list(set(tag_texts))
    return text_dict

## punlic API for duckduckgo
import requests
from bs4 import BeautifulSoup
import pandas as pd
import tldextract

# Specify your search query
query = 'IKEA'

# Construct the API request URL
url = f'https://api.duckduckgo.com/?q={query}&format=json'

# Send the API request
response = requests.get(url)

# Parse the JSON response
data = response.json()

# Extract relevant information from the response
if 'Type' in data and data['Type'] == 'E':
    print('Error:', data['Message'])
else:
    print('Results:')
    for result in data['Results']:
        print('Title:', result['FirstURL'])
        print('URL:', result['FirstURL'])
        print('Text:', result['Text'])
        print('-----------------------------------')

# Define the starting URL for crawling
start_url = data['Results'][0]["FirstURL"]

# Define a set to store visited URLs to avoid duplicates
visited_urls = set()
extracted_url = []
extract_domain = []
para_list = []
div_list = []
# Define a function to crawl a URL and extract relevant information
count = 0
def crawl(url):
    # Check if the URL has already been visited
    global count
    if url in visited_urls or count >= 1000:
        return

    # Fetch the web page content
    response = requests.get(url)
    if response.status_code == 200:
        # Parse the HTML content using BeautifulSoup
        soup = BeautifulSoup(response.content, "html.parser")
        
        # Extract relevant information from the web page
        # For example, extracting all the links on the page
        links = soup.find_all("a")
        for link in links:
            link_url = link.get("href")
            if link_url:
              # Recursively crawl the extracted URL
              if link_url.startswith("https://"):
              # Process the extracted URL (e.g., store in a database, add to a queue)
                if link_url in extracted_url:
                  continue
                else:
                  # print(link_url)
                  para_dict = get_tag_text(url=link_url, page_source_text=soup, tag_text="p")
                  div_dict = get_tag_text(url=link_url, page_source_text=soup, tag_text="div")
                  extracted_url.append(link_url)
                  extract_domain.append(domain_extract(link_url))
                  para_list.append(para_dict)
                  div_list.append(div_dict)
                  # increment count recersion  
                  count = count + 1
                  # recersion
                  crawl(link_url)

    # Add the URL to the set of visited URLs
    visited_urls.add(url)

    return visited_urls
# Start crawling from the starting URL
visited_url = crawl(url=start_url)
df = pd.DataFrame(data={"url":extracted_url,"domain":extract_domain, "tag_text_p":para_list, "tag_text_div":div_list})

df.head(50)

